{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HW 2: Bank Marketing**\n",
    "<hr>\n",
    "\n",
    "<u>**Members**</u><br>\n",
    "Tharnarch Thoranisttakul 63340500025<br>\n",
    "Athimet Aiewcharoen 63340500068\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions and Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OrdinalEncoder , OneHotEncoder\n",
    "\n",
    "# Fetures selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz , plot_tree \n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier  , ExtraTreesClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier , RadiusNeighborsClassifier, NearestCentroid\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Neural network\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Resampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold, RandomUnderSampler, NearMiss\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Make image directory (img)\n",
    "import os\n",
    "if not os.path.exists('img'):\n",
    "    os.mkdir('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder2():\n",
    "    def __init__(self, toEncode:list, columnToEncode:list):\n",
    "        self.toEncode = toEncode\n",
    "        self.columnToEncode = columnToEncode\n",
    "        \n",
    "    def transform(self,X,y=None,**transform_params):\n",
    "        for toEn, colToEn in zip(self.toEncode, self.columnToEncode):\n",
    "            X[colToEn] = X[colToEn].apply(lambda x: toEn.index(x))\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDummiesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A Dataframe transformer that provide dummy variable encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self, X, **transformparams):\n",
    "        \"\"\" Returns a dummy variable encoded version of a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        trans : pandas DataFrame\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        trans = pd.get_dummies(X).copy()\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        \"\"\" Do nothing operation\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Exploration**\n",
    "\n",
    "From the description file, there are a total of 17 features in this dataset which are:\n",
    "\n",
    "|Feature Name|Description|Data Type|\n",
    "|:-:|-|:-:|\n",
    "|<font color='green'>age</font>|Customer's age (in years)|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>job</font>|Types of Job|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>marital</font>|Marital Status|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>education</font>|Education|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>default</font>|Has credit in Default?|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>balance</font>|Average yearly balance|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>housing</font>|Has housing loan?|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>loan</font>|Has personal loan?|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>contact</font>|Contact Communication Type|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>day</font>|Last contact day of the month|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>month</font>|Last contact month of the year|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>duration</font>|Last contact duration (in seconds).<br><br> Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>campaign</font>|no. of contacts performed during this campaign and for this client|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>pdays</font>|no. of days that passed by after the client was last contacted from a previous campaign|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>previous</font>|no. of contacts performed before this campaign and for this client|<font color='red'>Numerical</font>|\n",
    "|<font color='green'>poutcome</font>|outcome of the previous marketing campaign|<font color='red'>Categorical</font>|\n",
    "|<font color='green'>y</font>|has the client subscribed a term deposit?|<font color='red'>Categorical</font> [Output]|\n",
    "\n",
    "Number of instances: 45211 for bank-full.csv <br>\n",
    "Number of Attributes: 16 + output attribute (17)<br>\n",
    "(45211 rows, 17 columns)\n",
    "\n",
    "Missing Attribute Values: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOri = pd.read_csv('../HWdata/bank/bank-full.csv', sep=';')\n",
    "df = dfOri.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset has 45211 instances and 17 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check and Remove Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the unnecessary features, we are left with 16 features including output (y).\n",
    "\n",
    "In which, there are:\n",
    "\n",
    "- 10 features, including output, that are '<font color='green'>object</font>', which means that they are '<font color='red'>Categorical Features</font>'.\n",
    "- 7 features that are '<font color='green'>int64</font>', which means that they are '<font color='red'>Numerical Features</font>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sort Features**\n",
    "**Order:**\n",
    "1. Numerical Features\n",
    "2. Categorical Features\n",
    "3. Output\n",
    "\n",
    "**Separate Numerical Features from Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNum = df.select_dtypes(include=['int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate Categorical Features from Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCate = df.select_dtypes(include=['object'])\n",
    "dfCate = dfCate.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate Output from Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOut = df.iloc[:,len(df.columns)-1:len(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate Separated Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfNum, dfCate, dfOut], axis=1)\n",
    "df['y'] = df['y'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pp.ProfileReport(df, title=\"Bank Full Report\")\n",
    "# profile.to_file('HW2OverallReport.html')\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot Features of Separated Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfyes = df[df['y'] == 1]\n",
    "dfno = df[df['y'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80, 80))\n",
    "for idx, col in enumerate(df.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(16, 16, idx+1)\n",
    "        sns.histplot(df[col], kde=False)\n",
    "        plt.title(col + ' (Base)')\n",
    "for idx, col in enumerate(dfyes.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(16, 16, idx+len(dfno.columns))\n",
    "        sns.histplot(dfno[col], kde=False)\n",
    "        plt.title(col + ' (no)')\n",
    "for idx, col in enumerate(dfno.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(16, 16, idx+(len(dfno.columns)*2)-1)\n",
    "        sns.histplot(dfyes[col], kde=False)\n",
    "        plt.title(col + ' (yes)')\n",
    "plt.savefig('img/AllHists.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will be taking some features that are noticeably different when plotting against output to re-plot it for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToPlot = ['education', 'housing', 'contact', 'poutcome']\n",
    "plt.figure(figsize=(25,25))\n",
    "for idx, col in enumerate(colToPlot):\n",
    "    plt.subplot(4, 4, idx+1)\n",
    "    sns.histplot(df[col], kde=False)\n",
    "    plt.title(col + ' (Full)')\n",
    "for idx, col in enumerate(colToPlot):\n",
    "    plt.subplot(4, 4, idx+len(colToPlot)+1)\n",
    "    sns.histplot(dfno[col], kde=False)\n",
    "    plt.title(col + ' (no)')\n",
    "for idx, col in enumerate(colToPlot):\n",
    "    plt.subplot(4, 4, idx+1+(len(colToPlot)*2))\n",
    "    sns.histplot(dfyes[col], kde=False)\n",
    "    plt.title(col + ' (yes)')\n",
    "plt.savefig('img/SelectedHists.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these features have different charts when plotting against each output. Features plotted against output 'yes' have different bar charts compared to features plotted against all output data while features plotted against output 'no' has the same bar charts.\n",
    "\n",
    "|Feature Names|Information|\n",
    "|-|-|\n",
    "|education|The client with 'secondary' education level has more chance to subscribe a term deposit|\n",
    "|housing|The client with 'no' housing loan has more chance to subscribe a term deposit|\n",
    "|contact|The client with 'unknown' communication type has more chance not to subscribe a term deposit|\n",
    "|poutcome|The client with 'success' outcome of the previous marketing campaign has more chance to subscribe a term deposit|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pairplotting Data Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.sample(int(df.shape[0]/10), random_state=777), diag_kind=\"kde\", hue='y')\n",
    "plt.savefig('img/PairplotSample.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see when pairplotting, variance between features are quite low, so that we can't see the difference between 'no' and 'yes' output, variance within class are quite high in some feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check for Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    plt.subplot(7, 7, idx+1)\n",
    "    sns.boxplot(x='y', y=col, data=df)\n",
    "    plt.title(col + ' (box)')\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    plt.subplot(7, 7, idx+len(dfNum.columns)+1)\n",
    "    sns.stripplot(x=\"y\", y=col, data=df)    \n",
    "    plt.title(col + ' (strip)')\n",
    "plt.savefig('img/BoxStrip.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, outliers can be seen in the following features:\n",
    "- balance\n",
    "- duration\n",
    "- campaign\n",
    "- previous\n",
    "\n",
    "We will remove the outliers at the start of data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Exploration's Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the overall report,\n",
    "\n",
    "Plotting Histogram of Numerical Features:\n",
    "\n",
    "|Feature Names|Distribution|\n",
    "|-|-|\n",
    "|age|Right-skewed|\n",
    "|balance|Right-skewed|\n",
    "|day|Uniform|\n",
    "|duration|Right-skewed|\n",
    "|campaign|Right-skewed|\n",
    "|pdays|Right-skewed|\n",
    "|previous|Right-skewed|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Values Count of Categorical Features:\n",
    "\n",
    "|Feature Names|What we got|\n",
    "|-|-|\n",
    "|job|Most common jobs are 'blue-collar', 'management' and 'technician' (59.2% in total).|\n",
    "|marital|60.2% of this data are 'Married', 28.3% are 'Single' and the rest are 'Divorced'|\n",
    "|education|Mostly 'secondary' (51.3%) followed by 'tertiary' (29.4%), 'primary' (15.2%) and 'unknown' (4.1%)|\n",
    "|default|Mostly 'false' (98.2%)|\n",
    "|housing|55.6% are 'true' and the rest are 'false'|\n",
    "|loan|Mostly 'false' (84%)|\n",
    "|contact|Mostly 'cellular' (64.8%) followed by 'unknown' (28.8%) and 'telephone' (6.4%)|\n",
    "|month|Mostly 'may' (30.4%)|\n",
    "|poutcome|Mostly 'unknown' (81.7%) followed by 'failure' (10.8%), 'other' (4.1%) and 'success' (3.3%)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good things of this dataset:\n",
    "\n",
    "|Good Things|Explanation|\n",
    "|-|-|\n",
    "|Filled Data in Numerical Features|There are no missing data (NaN) or unknown in numerical features of this dataset.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad things of this dataset:\n",
    "\n",
    "|Bad Things|Explanation|\n",
    "|-|-|\n",
    "|Data Explanation|Most features come with little to no explanation or bad explanation.|\n",
    "|Data Distribution|There are no normal distributed data. Almost all of them are right-skewed.|\n",
    "|Correlation|There are a lot of high correlation between features.|\n",
    "|'poutcome' Features|This feature's data are mostly 'unknown' (around 81.7%).|\n",
    "|Unknown Data|Unclear explanation of what unknown data means and there are a lot of them in categorical features.|\n",
    "|Data Variation|As we can see when pairplotting, variance between features are quite low and variance within class are quite high in some feature, which means that data variation is very bad.|\n",
    "|Output data|The output is imbalanced. ('1' is 11.7% and '0' is 88.3%)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: For all 'Unknown' data in categorical features, we assumed 'Unknown' as a type of data because there are quite a lot of 'Unknown' data.\n",
    "\n",
    "Furthermore, because we are dealing with a dataset from bank, those 'Unknown' data might be undisclosed information of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis**\n",
    "1. Outliers in features affects model's performance.\n",
    "2. Imbalance data affects model's performance.\n",
    "3. Right-skewed data affects model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **Benchmarking** (before data preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dummy variables for the categorical \n",
    "dfBM = df.copy()\n",
    "dfBM = pd.get_dummies(dfBM, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])\n",
    "\n",
    "xBM = dfBM.drop(['y'], axis = 1)\n",
    "yBM = dfBM['y']\n",
    "\n",
    "xBM_train, xBM_test, yBM_train, yBM_test = train_test_split(xBM, yBM, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf = DecisionTreeClassifier(random_state=0)\n",
    "DT_clf.fit(xBM_train,yBM_train)\n",
    "print('Train score: ', DT_clf.score(xBM_train, yBM_train))\n",
    "print('Test score: ', DT_clf.score(xBM_test, yBM_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Predict using Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDT_pred_test = DT_clf.predict(xBM_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yBM_test, y_pred=yDT_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yBM_test, y_pred=yDT_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-Layer Perceptron Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_clf = MLPClassifier(random_state=1)\n",
    "MLP_clf.fit(xBM_train,yBM_train)\n",
    "print('Train score: ', MLP_clf.score(xBM_train, yBM_train))\n",
    "print('Test score: ', MLP_clf.score(xBM_test, yBM_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Predict using Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMLP_pred_test = MLP_clf.predict(xBM_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yBM_test, y_pred=yMLP_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yBM_test, y_pred=yMLP_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "KN_clf.fit(xBM_train,yBM_train)\n",
    "print('Train score: ', KN_clf.score(xBM_train, yBM_train))\n",
    "print('Test score: ', KN_clf.score(xBM_test, yBM_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Predict using Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yKN_pred_test = KN_clf.predict(xBM_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yBM_test, y_pred=yKN_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yBM_test, y_pred=yKN_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Benchmark Conclusion**\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.92|0.93|0.87|Decision Tree|Test Data|\n",
    "1|0.45|0.47|0.46|^|Decision Tree|Test Data|\n",
    "0|0.94|0.92|0.93|0.88|MLP|Test Data|\n",
    "1|0.50|0.59|0.54|^|MLP|Test Data|\n",
    "0|0.91|0.96|0.93|0.88|K-Nearest|Test Data|\n",
    "1|0.46|0.25|0.33|^|K-Nearest|Test Data|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 1 : Remove Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hypothesis**\n",
    "    \n",
    "Outliers in features affects model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure,\n",
    "\n",
    "<img src='img/BoxStrip.png'/>\n",
    "\n",
    "we will be removing outliers from the following features:\n",
    "- balance\n",
    "- duration\n",
    "- campaign\n",
    "- previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEX1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('balance vs y (detailed)')\n",
    "plt.yticks(np.arange(0, 120000, 5000))\n",
    "plt.plot(np.arange(-1, 3, 1), np.full(4, 80000), 'g--')\n",
    "plt.plot(np.arange(0.5, 3, 1), np.full(3, 45000), 'g--')\n",
    "sns.stripplot(x=\"y\", y=\"balance\", data=dfEX1)\n",
    "plt.savefig('img/balance-y-detailed.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('duration vs y (detailed)')\n",
    "plt.yticks(np.arange(0, 5100, 100))\n",
    "plt.plot(np.arange(-1, 3, 1), np.full(4, 2800), 'g--')\n",
    "sns.stripplot(x=\"y\", y=\"duration\", data=dfEX1)\n",
    "plt.savefig('img/duration-y-detailed.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('campaign vs y (detailed)')\n",
    "plt.yticks(np.arange(0, 70, 1))\n",
    "plt.plot(np.arange(-1, 3, 1), np.full(4, 45), 'g--')\n",
    "plt.plot(np.arange(0.5, 3, 1), np.full(3, 18), 'g--')\n",
    "sns.stripplot(x=\"y\", y=\"campaign\", data=dfEX1)\n",
    "plt.savefig('img/campaign-y-detailed.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['previous'] > 60].index)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('previous vs y (detailed)')\n",
    "plt.yticks(np.arange(0, 50, 1))\n",
    "plt.plot(np.arange(-1, 3, 1), np.full(4, 30), 'g--')\n",
    "plt.plot(np.arange(0.5, 3, 1), np.full(3, 19), 'g--')\n",
    "sns.stripplot(x=\"y\", y=\"previous\", data=dfEX1)\n",
    "plt.savefig('img/previous-y-detailed.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['balance'] >= 50000].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['balance'] >= 45000].index & dfEX1[dfEX1['y'] == 1].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['duration'] >= 2800].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['campaign'] >= 45].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['campaign'] >= 18].index & dfEX1[dfEX1['y'] == 1].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['previous'] >= 30].index)\n",
    "dfEX1 = dfEX1.drop(dfEX1[dfEX1['previous'] >= 19].index & dfEX1[dfEX1['y'] == 1].index)\n",
    "\n",
    "dfEX1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers, we are left with <u>**45142 instances**</u> (originally 45211 instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for idx, col in enumerate(['balance', 'duration', 'campaign', 'previous']):\n",
    "    plt.subplot(2, 2, idx+1)\n",
    "    sns.stripplot(x='y', y=col, data=dfEX1)\n",
    "    plt.title(col + ' (after removing outliers)')\n",
    "plt.savefig('img/SelectedHists-After.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### **Benchmarking** (Remove Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dummy variables for the categorical \n",
    "dfEX1_BM = dfEX1.copy()\n",
    "dfEX1_BM = pd.get_dummies(dfEX1_BM, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])\n",
    "\n",
    "xEX1 = dfEX1_BM.drop(['y'], axis = 1)\n",
    "yEX1 = dfEX1_BM['y']\n",
    "\n",
    "xEX1_train, xEX1_test, yEX1_train, yEX1_test = train_test_split(xEX1, yEX1, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex1 = DecisionTreeClassifier(random_state=0)\n",
    "DT_clf_Ex1.fit(xEX1_train,yEX1_train)\n",
    "print('Train score: ', DT_clf_Ex1.score(xEX1_train, yEX1_train))\n",
    "print('Test score: ', DT_clf_Ex1.score(xEX1_test, yEX1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDT_pred_test = DT_clf_Ex1.predict(xEX1_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yEX1_test, y_pred=yDT_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yEX1_test, y_pred=yDT_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Multi-Layer Perceptron Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_clf_Ex1 = MLPClassifier(random_state=1)\n",
    "MLP_clf_Ex1.fit(xEX1_train,yEX1_train)\n",
    "print('Train score: ', MLP_clf_Ex1.score(xEX1_train, yEX1_train))\n",
    "print('Test score: ', MLP_clf_Ex1.score(xEX1_test, yEX1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMLP_pred_test = MLP_clf_Ex1.predict(xEX1_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yEX1_test, y_pred=yMLP_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yEX1_test, y_pred=yMLP_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_clf_Ex1 = KNeighborsClassifier(n_jobs=-1)\n",
    "KN_clf_Ex1.fit(xEX1_train,yEX1_train)\n",
    "print('Train score: ', KN_clf_Ex1.score(xEX1_train, yEX1_train))\n",
    "print('Test score: ', KN_clf_Ex1.score(xEX1_test, yEX1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yKN_pred_test = KN_clf_Ex1.predict(xEX1_test)\n",
    "\n",
    "print(\"Classification report on Test data\\n=======================\")\n",
    "print(classification_report(y_true=yEX1_test, y_pred=yKN_pred_test))\n",
    "\n",
    "print(\"Confusion matrix on Test data\\n=======================\")\n",
    "print(confusion_matrix(y_true=yEX1_test, y_pred=yKN_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "From the result of this experiment, we can compare the model's performance before and after removing outliers.\n",
    "\n",
    "1. Decision Tree  \n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.93|0.92|0.93|0.87|Decision Tree|Test Data|\n",
    "|1|0.45|0.47|0.46|^|Decision Tree|Test Data|\n",
    "|0|0.93|0.93|0.93|0.90|Decision Tree|Test Ex1 Data|\n",
    "|1|0.50|0.51|0.51|^|Decision Tree|Test Ex1 Data|\n",
    "\n",
    "2. MLP  \n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.93|0.94|0.94|0.89|MLP|Test Data|\n",
    "|1|0.52|0.49|0.51|^|MLP|Test Data|\n",
    "|0|0.93|0.95|0.94|0.89|MLP|Test Ex1 Data|\n",
    "|1|0.55|0.42|0.48|^|MLP|Test Ex1 Data|\n",
    "\n",
    "3. K-Nearest  \n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.91|0.96|0.93|0.88|K-Nearest|Test Data|\n",
    "|1|0.46|0.25|0.33|^|K-Nearest|Test Data|\n",
    "|0|0.91|0.96|0.93|0.88|K-Nearest|Test Ex1 Data|\n",
    "|1|0.48|0.27|0.34|^|K-Nearest|Test Ex1 Data|\n",
    "\n",
    "From the result above, we can see that removing outliers from the data can improve the model's performance. Therefore, we will be removing outliers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfEX1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 2 : Data Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hypothesis**\n",
    "    \n",
    "Imbalance data affects model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toEncode = [[\"no\",\"yes\"], [\"no\",\"yes\"], [\"no\",\"yes\"], [\"unknown\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]]\n",
    "columnToEncode = ['default', 'housing', 'loan', 'month']\n",
    "columnToOnehot = ['job', 'marital', 'education', 'contact', 'poutcome']\n",
    "\n",
    "LE_pipeline = Pipeline([\n",
    "    (\"LabelEncoder\", LabelEncoder2(toEncode, columnToEncode)),\n",
    "    (\"OneHotEncoder\", ToDummiesTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(df.groupby('y').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI = df.copy()\n",
    "dfDI = LE_pipeline.fit_transform(dfDI)\n",
    "\n",
    "xdi_train , xdi_test , ydi_train , ydi_test = train_test_split(dfDI.drop(['y'], axis = 1), dfDI['y'], test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, We consider to use 6 different methods to address the imbalance data problem.\n",
    "1. Random Oversampling\n",
    "2. SMOTE\n",
    "3. ADASYN\n",
    "4. Random Undersampling\n",
    "5. InstanceHardnessThreshold\n",
    "6. NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "x_ros, y_ros = ros.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_Ros = pd.concat([x_ros, y_ros], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_Ros.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_Ros.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=0)\n",
    "x_su, y_su = smote.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_SM = pd.concat([x_su, y_su], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_SM.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_SM.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ADASYN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyb = ADASYN(random_state=0)\n",
    "x_ad, y_ad = adasyb.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_AD = pd.concat([x_ad, y_ad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_AD.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_AD.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "x_rus , y_rus = rus.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_RUS = pd.concat([x_rus, y_rus], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_RUS.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_RUS.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **InstanceHardnessThreshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instanace = InstanceHardnessThreshold(n_jobs=-1)\n",
    "x_iht , y_iht = instanace.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_IHT = pd.concat([x_iht, y_iht], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_IHT.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_IHT.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NearMiss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearMiss = NearMiss(n_jobs=-1)\n",
    "x_nearM , y_nearM = nearMiss.fit_resample(xdi_train, ydi_train)\n",
    "dfDI_NearM = pd.concat([x_nearM, y_nearM], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDI_NearM.groupby('y').size().plot(kind='pie',\n",
    "                            y = \"y\",\n",
    "                            label = \"Type\",\n",
    "                            autopct='%1.1f%%')\n",
    "print(dfDI_NearM.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Histogram Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(df[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Base)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_Ros[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Random Over Sampling)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_SM[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (SMOTE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_AD[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (ADASYN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_RUS[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Random Under Sampling)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_IHT[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Instance Hardness Threshold)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(dfDI_NearM[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Near Miss)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in some features(duration,day), the data not have the same distribution as the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_Base = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_Base = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_Base = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_Base.fit(xdi_train, ydi_train)\n",
    "MLP_clf_Ex2_Base.fit(xdi_train, ydi_train)\n",
    "KN_clf_Ex2_Base.fit(xdi_train, ydi_train)\n",
    "print(\"Decision Tree Classifier Base: \", DT_clf_Ex2_Base.score(xdi_test, ydi_test))\n",
    "print(\"MLP Classifier Base: \", MLP_clf_Ex2_Base.score(xdi_test, ydi_test))\n",
    "print(\"KNeighbors Classifier Base: \", KN_clf_Ex2_Base.score(xdi_test, ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_Base.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_Base.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_Base.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Random Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_ros = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_ros = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_ros = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_ros.fit(x_ros,y_ros)\n",
    "MLP_clf_Ex2_ros.fit(x_ros,y_ros)\n",
    "KN_clf_Ex2_ros.fit(x_ros,y_ros)\n",
    "print(\"Score of Decision Tree Classifier on test data with Random Over Sampling: \",DT_clf_Ex2_ros.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with Random Over Sampling: \",MLP_clf_Ex2_ros.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with Random Over Sampling: \",KN_clf_Ex2_ros.score(xdi_test,ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_ros.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_ros.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_ros.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_sm = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_sm = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_sm = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_sm.fit(x_su,y_su)\n",
    "MLP_clf_Ex2_sm.fit(x_su,y_su)\n",
    "KN_clf_Ex2_sm.fit(x_su,y_su)\n",
    "print(\"Score of Decision Tree Classifier on test data with SMOTE: \",DT_clf_Ex2_sm.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with SMOTE: \",MLP_clf_Ex2_sm.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with SMOTE: \",KN_clf_Ex2_sm.score(xdi_test,ydi_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_sm.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_sm.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_sm.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ADASYN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_ad = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_ad = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_ad = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_ad.fit(x_ad,y_ad)\n",
    "MLP_clf_Ex2_ad.fit(x_ad,y_ad)\n",
    "KN_clf_Ex2_ad.fit(x_ad,y_ad)\n",
    "print(\"Score of Decision Tree Classifier on test data with ADASYN: \",DT_clf_Ex2_ad.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with ADASYN: \",MLP_clf_Ex2_ad.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with ADASYN: \",KN_clf_Ex2_ad.score(xdi_test,ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_ad.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_ad.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_ad.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Random Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_rus = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_rus = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_rus = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_rus.fit(x_rus,y_rus)\n",
    "MLP_clf_Ex2_rus.fit(x_rus,y_rus)\n",
    "KN_clf_Ex2_rus.fit(x_rus,y_rus)\n",
    "print(\"Score of Decision Tree Classifier on test data with Random Under Sampling: \",DT_clf_Ex2_rus.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with Random Under Sampling: \",MLP_clf_Ex2_rus.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with Random Under Sampling: \",KN_clf_Ex2_rus.score(xdi_test,ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_rus.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_rus.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_rus.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **InstanceHardnessThreshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_iht = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_iht = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_iht = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_iht.fit(x_iht,y_iht)\n",
    "MLP_clf_Ex2_iht.fit(x_iht,y_iht)\n",
    "KN_clf_Ex2_iht.fit(x_iht,y_iht)\n",
    "print(\"Score of Decision Tree Classifier on test data with Instance Hardness Threshold: \",DT_clf_Ex2_iht.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with Instance Hardness Threshold: \",MLP_clf_Ex2_iht.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with Instance Hardness Threshold: \",KN_clf_Ex2_iht.score(xdi_test,ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_iht.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_iht.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_iht.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NearMiss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_Ex2_nm = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_Ex2_nm = MLPClassifier(random_state=1)\n",
    "KN_clf_Ex2_nm = KNeighborsClassifier(n_jobs=-1)\n",
    "DT_clf_Ex2_nm.fit(x_nearM,y_nearM)\n",
    "MLP_clf_Ex2_nm.fit(x_nearM,y_nearM)\n",
    "KN_clf_Ex2_nm.fit(x_nearM,y_nearM)\n",
    "print(\"Score of Decision Tree Classifier on test data with Near Miss: \",DT_clf_Ex2_nm.score(xdi_test,ydi_test))\n",
    "print(\"Score of MLP Classifier on test data with Near Miss: \",MLP_clf_Ex2_nm.score(xdi_test,ydi_test))\n",
    "print(\"Score of KNN Classifier on test data with Near Miss: \",KN_clf_Ex2_nm.score(xdi_test,ydi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDI_pred_test1 = DT_clf_Ex2_nm.predict(xdi_test)\n",
    "yDI_pred_test2 = MLP_clf_Ex2_nm.predict(xdi_test)\n",
    "yDI_pred_test3 = KN_clf_Ex2_nm.predict(xdi_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=ydi_test, y_pred=yDI_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.92|0.93|0.88|Decision Tree|No Resampling|\n",
    "1|0.49|0.51|0.50|^|Decision Tree|No Resampling|\n",
    "0|0.94|0.91|0.93|0.87|MLP|No Resampling|\n",
    "1|0.46|0.59|0.52|^|MLP|No Resampling|\n",
    "0|0.91|0.96|0.93|0.88|K-Nearest|No Resampling|\n",
    "1|0.48|0.27|0.35|^|K-Nearest|No Resampling|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.93|0.93|0.88|Decision Tree|Random Oversampling|\n",
    "1|0.48|0.45|0.46|^|Decision Tree|Random Oversamplinga|\n",
    "0|0.96|0.82|0.88|0.81|MLP|Random Oversampling|\n",
    "1|0.35|0.73|0.47|^|MLP|Random Oversampling|\n",
    "0|0.94|0.82|0.87|0.79|K-Nearest|Random Oversampling|\n",
    "1|0.31|0.60|0.41|^|K-Nearest|Random Oversampling|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.92|0.92|0.87|Decision Tree|SMOTE|\n",
    "1|0.45|0.49|0.47|^|Decision Tree|SMOTE|\n",
    "0|0.93|0.95|0.94|0.89|MLP|SMOTE|\n",
    "1|0.53|0.45|0.48|^|MLP|SMOTE|\n",
    "0|0.94|0.79|0.86|0.77|K-Nearest|SMOTE|\n",
    "1|0.29|0.62|0.39|^|K-Nearest|SMOTE|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.91|0.92|0.86|Decision Tree|ADASYN|\n",
    "1|0.44|0.50|0.46|^|Decision Tree|ADASYN|\n",
    "0|0.91|0.98|0.94|0.89|MLP|ADASYN|\n",
    "1|0.64|0.24|0.35|^|MLP|ADASYN|\n",
    "0|0.94|0.76|0.84|0.75|K-Nearest|ADASYN|\n",
    "1|0.27|0.65|0.38|^|K-Nearest|ADASYN|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.96|0.80|0.87|0.80|Decision Tree|Random Undersampling|\n",
    "1|0.34|0.78|0.48|^|Decision Tree|Random Undersampling|\n",
    "0|0.93|0.93|0.93|0.87|MLP|Random Undersampling|\n",
    "1|0.46|0.47|0.47|^|MLP|Random Undersampling|\n",
    "0|0.95|0.76|0.85|0.76|K-Nearest|Random Undersampling|\n",
    "1|0.29|0.73|0.41|^|K-Nearest|Random Undersampling|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.99|0.69|0.82|0.72|Decision Tree|Instance Hardness Threshold|\n",
    "1|0.29|0.96|0.45|^|Decision Tree|Instance Hardness Threshold|\n",
    "0|0.98|0.63|0.77|0.67|MLP|Instance Hardness Threshold|\n",
    "1|0.25|0.92|0.40|^|MLP|Instance Hardness Threshold|\n",
    "0|0.96|0.78|0.86|0.78|K-Nearest|Instance Hardness Threshold|\n",
    "1|0.31|0.73|0.43|^|K-Nearest|Instance Hardness Threshold|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.97|0.39|0.56|0.45|Decision Tree|Near Miss|\n",
    "1|0.17|0.91|0.28|^|Decision Tree|Near Miss|\n",
    "0|0.95|0.50|0.65|0.54|MLP|Near Miss|\n",
    "1|0.18|0.82|0.29|^|MLP|Near Miss|\n",
    "0|0.96|0.41|0.58|0.47|K-Nearest|Near Miss|\n",
    "1|0.16|0.86|0.28|^|K-Nearest|Near Miss|\n",
    "\n",
    "Since resampling makes the data have more or less instances, we have to train the model again. Therefore, we trained the model again with the 3 models that we chose before, which are Decision Tree, MLP, K-Nearest, using the data that we resampled. However, the results showed that resampling our data set will reduce the performance of our model in some cases or not increase in some cases. In addition, resampling multiple times will change the histogram of the data.\n",
    "\n",
    "Therefore, we will not consider using the data that we resampled to train the model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 3 : Data Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hypothesis**\n",
    "    \n",
    "Right-skewed data affects model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate(dfNum.columns):\n",
    "    if col != 'y':\n",
    "        plt.subplot(1, 7, idx+1)\n",
    "        sns.histplot(df[col], kde=True,bins=20)\n",
    "        plt.title(col + ' (Base)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is right-skewed, we will try to transform the data to make it more normal distribution. We will try to transform the data using log, square root, and cube root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Age Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_base = df['age']\n",
    "age_log = np.log(df[\"age\"])\n",
    "age_sqrt = np.sqrt(df[\"age\"])\n",
    "age_cbrt = np.cbrt(df[\"age\"])\n",
    "title = ['age', 'log(age)', 'sqrt(age)', 'cbrt(age)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([age_base, age_log, age_sqrt, age_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Balance Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_base = df['balance']\n",
    "balance_log = np.log(df[\"balance\"])\n",
    "balance_sqrt = np.sqrt(df[\"balance\"])\n",
    "balance_cbrt = np.cbrt(df[\"balance\"])\n",
    "title = ['balance', 'log(balance)', 'sqrt(balance)', 'cbrt(balance)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([balance_base, balance_log, balance_sqrt, balance_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Day Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_base = df['day']\n",
    "day_log = np.log(df[\"day\"])\n",
    "day_sqrt = np.sqrt(df[\"day\"])\n",
    "day_cbrt = np.cbrt(df[\"day\"])\n",
    "title = ['day', 'log(day)', 'sqrt(day)', 'cbrt(day)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([day_base, day_log, day_sqrt, day_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Duration Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_base = df['duration']\n",
    "duration_log = np.log(df[\"duration\"])\n",
    "duration_sqrt = np.sqrt(df[\"duration\"])\n",
    "duration_cbrt = np.cbrt(df[\"duration\"])\n",
    "title = ['duration', 'log(duration)', 'sqrt(duration)', 'cbrt(duration)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([duration_base, duration_log, duration_sqrt, duration_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Campaign Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_base = df['campaign']\n",
    "campaign_log = np.log(df[\"campaign\"])\n",
    "campaign_sqrt = np.sqrt(df[\"campaign\"])\n",
    "campaign_cbrt = np.cbrt(df[\"campaign\"])\n",
    "title = ['campaign', 'log(campaign)', 'sqrt(campaign)', 'cbrt(campaign)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([campaign_base, campaign_log, campaign_sqrt, campaign_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pdays Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdays_base = df['pdays']\n",
    "pdays_log = np.log(df[\"pdays\"])\n",
    "pdays_sqrt = np.sqrt(df[\"pdays\"])\n",
    "pdays_cbrt = np.cbrt(df[\"pdays\"])\n",
    "title = ['pdays', 'log(pdays)', 'sqrt(pdays)', 'cbrt(pdays)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([pdays_base, pdays_log, pdays_sqrt, pdays_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Previous Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_base = df['previous']\n",
    "previous_log = np.log(df[\"previous\"])\n",
    "previous_sqrt = np.sqrt(df[\"previous\"])\n",
    "previous_cbrt = np.cbrt(df[\"previous\"])\n",
    "title = ['previous', 'log(previous)', 'sqrt(previous)', 'cbrt(previous)']\n",
    "plt.figure(figsize=(25, 5))\n",
    "for idx, col in enumerate([previous_base, previous_log, previous_sqrt, previous_cbrt]):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    sns.histplot(col, kde=True,bins=20)\n",
    "    plt.title(title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the graph above, we can see that some of the features after transformed are more normal distribution than before. However, some of the features are still right-skewed. so we will try to use the features that are more normal distribution than before. which are \n",
    "1.Age = Log\n",
    "2.Balance = cbrt(followed by Log)\n",
    "3.Duration = cbrt(followed by Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toEncode = [[\"no\",\"yes\"], [\"no\",\"yes\"], [\"no\",\"yes\"], [\"unknown\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]]\n",
    "columnToEncode = ['default', 'housing', 'loan', 'month']\n",
    "columnToOnehot = ['job', 'marital', 'education', 'contact', 'poutcome']\n",
    "\n",
    "LE_pipeline = Pipeline([\n",
    "    (\"LabelEncoder\", LabelEncoder2(toEncode, columnToEncode)),\n",
    "    (\"OneHotEncoder\", ToDummiesTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDT = df.copy()\n",
    "dfDT = LE_pipeline.fit_transform(dfDT)\n",
    "dfDT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDT_X = dfDT.copy()\n",
    "dfDT_X.drop('y', axis=1, inplace=True)\n",
    "dfDT_Y = dfDT['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_train, x_tf_test, y_tf_train, y_tf_test = train_test_split(dfDT_X, dfDT_Y,  test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Nontransformed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DT_N = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DT_N = MLPClassifier(random_state=1)\n",
    "KN_clf_DT_N = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DT_N.fit(x_tf_train, y_tf_train)\n",
    "MLP_clf_DT_N.fit(x_tf_train, y_tf_train)\n",
    "KN_clf_DT_N.fit(x_tf_train, y_tf_train)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DT_N.score(x_tf_test, y_tf_test))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DT_N.score(x_tf_test, y_tf_test))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DT_N.score(x_tf_test, y_tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDT_pred_test1 = DT_clf_DT_N.predict(x_tf_test)\n",
    "yDT_pred_test2 = MLP_clf_DT_N.predict(x_tf_test)\n",
    "yDT_pred_test3 = KN_clf_DT_N.predict(x_tf_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Transformed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_train_T = x_tf_train.copy()\n",
    "x_tf_test_T = x_tf_test.copy()\n",
    "x_tf_train_T[\"age\"] , x_tf_test_T[\"age\"] = np.log(x_tf_train[\"age\"]) , np.log(x_tf_test[\"age\"])\n",
    "x_tf_train_T[\"balance\"] , x_tf_test_T[\"balance\"] = np.cbrt(x_tf_train[\"balance\"]) , np.cbrt(x_tf_test[\"balance\"])\n",
    "x_tf_train_T[\"duration\"] , x_tf_test_T[\"duration\"] = np.cbrt(x_tf_train[\"duration\"]) , np.cbrt(x_tf_test[\"duration\"])\n",
    "x_tf_train_T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DT_T = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DT_T = MLPClassifier(random_state=1)\n",
    "KN_clf_DT_T = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DT_T.fit(x_tf_train_T, y_tf_train)\n",
    "MLP_clf_DT_T.fit(x_tf_train_T, y_tf_train)\n",
    "KN_clf_DT_T.fit(x_tf_train_T, y_tf_train)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DT_T.score(x_tf_test_T, y_tf_test))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DT_T.score(x_tf_test_T, y_tf_test))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DT_T.score(x_tf_test_T, y_tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDT_pred_test1 = DT_clf_DT_T.predict(x_tf_test_T)\n",
    "yDT_pred_test2 = MLP_clf_DT_T.predict(x_tf_test_T)\n",
    "yDT_pred_test3 = KN_clf_DT_T.predict(x_tf_test_T)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_tf_test, y_pred=yDT_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.92|0.93|0.88|Decision Tree|Not Tranformed|\n",
    "1|0.49|0.51|0.50|^|Decision Tree|Not Tranformed|\n",
    "0|0.94|0.91|0.93|0.87|MLP|Not Tranformed|\n",
    "1|0.46|0.59|0.52|^|MLP|Not Tranformed|\n",
    "0|0.91|0.96|0.93|0.88|K-Nearest|Not Tranformed|\n",
    "1|0.48|0.27|0.35|^|K-Nearest|Not Tranformed|\n",
    "\n",
    "<br>\n",
    "\n",
    "Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "0|0.93|0.92|0.93|0.88|Decision Tree|Tranformed|\n",
    "1|0.49|0.51|0.50|^|Decision Tree|Tranformed|\n",
    "0|0.82|0.97|0.94|0.90|MLP|Tranformed|\n",
    "1|0.62|0.34|0.44|^|MLP|Tranformed|\n",
    "0|0.91|0.97|0.94|0.89|K-Nearest|Tranformed|\n",
    "1|0.58|0.32|0.41|^|K-Nearest|Tranformed|\n",
    "\n",
    "From data transformation, we can see that the data is distributed differently in some features. Therefore, we will train the model again with the 3 models that we have chosen before, which are Decision Tree, MLP, K-Nearest, using the transformed data. However, the results showed that when we transformed the data, the performance of the model will change into 3 different forms with all 3 models as follows:\n",
    "1. Decision Tree has similar performance as the model trained with the data that was not transformed.\n",
    "2. K-Nearest has better performance than the model trained with the data that was not transformed.\n",
    "3. MLP has lower performance than the model trained with the data that was not transformed.\n",
    "\n",
    "Therefore, we will use the data that was transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDT[\"age\"] = np.log(dfDT[\"age\"])\n",
    "dfDT['balance'] = np.cbrt(dfDT['balance'])\n",
    "dfDT['duration'] = np.cbrt(dfDT['duration'])\n",
    "\n",
    "df = dfDT.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 4 : Data Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDS = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dfDS.drop(['y'], axis = 1), dfDS['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMaxScale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "StandardScale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "RobustScale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMMS = pd.concat([X, y], axis=1)\n",
    "dfSS = pd.concat([X, y], axis=1)\n",
    "dfRS = pd.concat([X, y], axis=1)\n",
    "\n",
    "for idx in dfDS.columns:\n",
    "    if idx not in ['y']:\n",
    "        dfMMS[idx] = MinMaxScale.fit_transform(X[idx].values.reshape(-1,1))\n",
    "        dfSS[idx] = StandardScale.fit_transform(X[idx].values.reshape(-1,1))\n",
    "        dfRS[idx] = RobustScale.fit_transform(X[idx].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XMMS, yMMS = dfMMS.drop(['y'], axis = 1), dfMMS['y']\n",
    "XSS, ySS = dfSS.drop(['y'], axis = 1), dfSS['y']\n",
    "XRS, yRS = dfRS.drop(['y'], axis = 1), dfRS['y']\n",
    "\n",
    "xMMS_train, xMMS_test, yMMS_train, yMMS_test = train_test_split(XMMS, yMMS, test_size=0.2, random_state=1)\n",
    "xSS_train, xSS_test, ySS_train, ySS_test = train_test_split(XSS, ySS, test_size=0.2, random_state=1)\n",
    "xRS_train, xRS_test, yRS_train, yRS_test = train_test_split(XRS, yRS, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decision Tree Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MMS = DecisionTreeClassifier(random_state=1)\n",
    "DT_SS = DecisionTreeClassifier(random_state=1)\n",
    "DT_RS = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "DT_MMS = DT_MMS.fit(xMMS_train,yMMS_train)\n",
    "DT_SS = DT_SS.fit(xSS_train,ySS_train)\n",
    "DT_RS = DT_RS.fit(xRS_train,yRS_train)\n",
    "\n",
    "print(\"Decision Tree Classifier Accuracy with MinMaxScaler: \", DT_MMS.score(xMMS_test, yMMS_test))\n",
    "print(\"Decision Tree Classifier Accuracy with StandardScaler: \", DT_SS.score(xSS_test, ySS_test))\n",
    "print(\"Decision Tree Classifier Accuracy with RobustScaler: \", DT_RS.score(xRS_test, yRS_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDTMMS_pred_test = DT_MMS.predict(xMMS_test)\n",
    "yDTSS_pred_test = DT_SS.predict(xSS_test)\n",
    "yDTRS_pred_test = DT_RS.predict(xRS_test)\n",
    "\n",
    "print(\"Classification report on Decision Tree Classifier Accuracy with MinMaxScaler\\n=======================\")\n",
    "print(classification_report(y_true=yMMS_test, y_pred=yDTMMS_pred_test))\n",
    "print(\"Classification report on Decision Tree Classifier Accuracy with StandardScaler\\n=======================\")\n",
    "print(classification_report(y_true=ySS_test, y_pred=yDTSS_pred_test))\n",
    "print(\"Classification report on Decision Tree Classifier Accuracy with RobustScaler\\n=======================\")\n",
    "print(classification_report(y_true=yRS_test, y_pred=yDTRS_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Multi-Layer Perceptron Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_MMS = MLPClassifier(random_state=1)\n",
    "MLP_SS = MLPClassifier(random_state=1)\n",
    "MLP_RS = MLPClassifier(random_state=1)\n",
    "MLP_MMS = MLP_MMS.fit(xMMS_train,yMMS_train)\n",
    "MLP_SS = MLP_SS.fit(xSS_train,ySS_train)\n",
    "MLP_RS = MLP_RS.fit(xRS_train,yRS_train)\n",
    "\n",
    "print(\"MLP Classifier Accuracy with MinMaxScaler: \", MLP_MMS.score(xMMS_test, yMMS_test))\n",
    "print(\"MLP Classifier Accuracy with StandardScaler: \", MLP_SS.score(xSS_test, ySS_test))\n",
    "print(\"MLP Classifier Accuracy with RobustScaler: \", MLP_RS.score(xRS_test, yRS_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMLPMMS_pred_test = MLP_MMS.predict(xMMS_test)\n",
    "yMLPSS_pred_test = MLP_SS.predict(xSS_test)\n",
    "yMLPRS_pred_test = MLP_RS.predict(xRS_test)\n",
    "\n",
    "print(\"Classification report on MLP Classifier Accuracy with MinMaxScaler\\n=======================\")\n",
    "print(classification_report(y_true=yMMS_test, y_pred=yMLPMMS_pred_test))\n",
    "print(\"Classification report on MLP Classifier Accuracy with StandardScaler\\n=======================\")\n",
    "print(classification_report(y_true=ySS_test, y_pred=yMLPSS_pred_test))\n",
    "print(\"Classification report on MLP Classifier Accuracy with RobustScaler\\n=======================\")\n",
    "print(classification_report(y_true=yRS_test, y_pred=yMLPRS_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_MMS = KNeighborsClassifier(n_jobs=-1)\n",
    "KNN_SS = KNeighborsClassifier(n_jobs=-1)\n",
    "KNN_RS = KNeighborsClassifier(n_jobs=-1)\n",
    "KNN_MMS = KNN_MMS.fit(xMMS_train,yMMS_train)\n",
    "KNN_SS = KNN_SS.fit(xSS_train,ySS_train)\n",
    "KNN_RS = KNN_RS.fit(xRS_train,yRS_train)\n",
    "\n",
    "print(\"KNN Classifier Accuracy with MinMaxScaler: \", KNN_MMS.score(xMMS_test, yMMS_test))\n",
    "print(\"KNN Classifier Accuracy with StandardScaler: \", KNN_SS.score(xSS_test, ySS_test))\n",
    "print(\"KNN Classifier Accuracy with RobustScaler: \", KNN_RS.score(xRS_test, yRS_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yKNNMMS_pred_test = KNN_MMS.predict(xMMS_test)\n",
    "yKNNSS_pred_test = KNN_SS.predict(xSS_test)\n",
    "yKNNRS_pred_test = KNN_RS.predict(xRS_test)\n",
    "\n",
    "print(\"Classification report on KNN Classifier Accuracy with MinMaxScaler\\n=======================\")\n",
    "print(classification_report(y_true=yMMS_test, y_pred=yKNNMMS_pred_test))\n",
    "print(\"Classification report on KNN Classifier Accuracy with StandardScaler\\n=======================\")\n",
    "print(classification_report(y_true=ySS_test, y_pred=yKNNSS_pred_test))\n",
    "print(\"Classification report on KNN Classifier Accuracy with RobustScaler\\n=======================\")\n",
    "print(classification_report(y_true=yRS_test, y_pred=yKNNRS_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Scaler|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.93|0.93|0.93|0.87|Decision Tree|Min-Max Scaler|\n",
    "|1|0.47|0.48|0.47|^|Decision Tree|Min-Max Scaler|\n",
    "|0|0.93|0.93|0.93|0.87|Decision Tree|Standard Scaler|\n",
    "|1|0.47|0.47|0.47|^|Decision Tree|Standard Scaler|\n",
    "|0|0.93|0.93|0.93|0.87|Decision Tree|Robust Scaler|\n",
    "|1|0.47|0.48|0.47|^|Decision Tree|Robust Scaler|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Scaler|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.92|0.96|0.94|0.89|MLP|Min-Max Scaler|\n",
    "|1|0.60|0.38|0.46|^|MLP|Min-Max Scaler|\n",
    "|0|0.92|0.96|0.94|0.89|MLP|Standard Scaler|\n",
    "|1|0.58|0.40|0.47|^|MLP|Standard Scaler|\n",
    "|0|0.93|0.97|0.95|0.90|MLP|Robust Scaler|\n",
    "|1|0.64|0.44|0.52|^|MLP|Robust Scaler|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Scaler|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.90|0.97|0.94|0.89|KNN|Min-Max Scaler|\n",
    "|1|0.56|0.26|0.35|^|KNN|Min-Max Scaler|\n",
    "|0|0.91|0.97|0.94|0.89|KNN|Standard Scaler|\n",
    "|1|0.57|0.29|0.38|^|KNN|Standard Scaler|\n",
    "|0|0.91|0.97|0.94|0.89|KNN|Robust Scaler|\n",
    "|1|0.58|0.29|0.38|^|KNN|Robust Scaler|\n",
    "\n",
    "From the experiment, we can see that, in every model, the results show us that dataset scaled with Robust Scaler scored slightly better than other scalers. This is because, according to the scalers' meanings, 'Robust Scaler' is for datasets with skewed distribution and outliers, while Min-Max Scaler is for datasets that isn't distorted (this dataset is distorted) and Standard Scaler is for datasets that conforms to normal distribution.\n",
    "\n",
    "Therefore, we will use Robust Scaler to scale our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfRS.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 5 : Drop Feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hypothesis**\n",
    "    \n",
    "We will drop some features that we think are not important to the model or have a low correlation with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkCorrFF(df):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    cor = df.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap='inferno')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDF = df.copy()\n",
    "x = dfDF.drop('y', axis=1)\n",
    "y = dfDF['y']\n",
    "dfDF = pd.concat([x,y], axis=1)\n",
    "dfDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCorrFF(dfDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have many features, we will drop the features fist, with ANOVA method(beacause we have both categorical and numerical features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ANOVA method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=12)\n",
    "x_sa = dfDF.drop(['y'], axis = 1)\n",
    "y_sa = dfDF['y']\n",
    "\n",
    "dfDF = selector.fit(x_sa, y_sa)\n",
    "\n",
    "col = selector.get_support(indices=True)\n",
    "dfDF = x_sa.iloc[:,col]\n",
    "dfDF = pd.concat([dfDF, y_sa], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCorrFF(dfDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix, we can see that there are some features that have a low correlation with the target. We will drop these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fd_train , x_fd_test , y_fd_train , y_fd_test = train_test_split(dfDF.drop(['y'], axis = 1), dfDF['y'], test_size=0.2, random_state=0)\n",
    "dfBase = dfDF.copy()\n",
    "dfDF = pd.concat([x_fd_train, y_fd_train], axis=1)\n",
    "dfDF_Test = pd.concat([x_fd_test, y_fd_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will consider dropping some features that have a low correlation with the target anmd have a high correlation with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSep = dfOri.copy()\n",
    "\n",
    "dfContact1, dfContact2 = dfSep[dfSep['contact'] == 'unknown'], dfSep[dfSep['contact'] == 'cellular']\n",
    "dfContact = pd.concat([dfContact1, dfContact2], axis=0)\n",
    "dfContactYes = dfContact[dfContact['y'] == 'yes']\n",
    "dfContactNo = dfContact[dfContact['y'] == 'no']\n",
    "\n",
    "dfPoutcome1, dfPoutcome2 = dfSep[dfSep['poutcome'] == 'unknown'], dfSep[dfSep['poutcome'] == 'success']\n",
    "dfPoutcome = pd.concat([dfPoutcome1, dfPoutcome2], axis=0)\n",
    "dfPoutcomeYes = dfPoutcome[dfPoutcome['y'] == 'yes']\n",
    "dfPoutcomeNo = dfPoutcome[dfPoutcome['y'] == 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.histplot(dfContact['contact'], kde=False)\n",
    "plt.title('Contact (Full)')\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.histplot(dfContactNo['contact'], kde=False)\n",
    "plt.title('Contact (No)')\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.histplot(dfContactYes['contact'], kde=False)\n",
    "plt.title('Contact (Yes)')\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.histplot(dfPoutcome['poutcome'], kde=False)\n",
    "plt.title('poutcome (Full)')\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(dfPoutcomeNo['poutcome'], kde=False)\n",
    "plt.title('poutcome (No)')\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.histplot(dfPoutcomeYes['poutcome'], kde=False)\n",
    "plt.title('poutcome (Yes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this figure, we can see that there are \"contact\" and \"poutcome\" have different histograms in the separated target class. In \"contact\", 'unknown' is not visually similar in 'yes' and 'no'. In \"poutcome\", 'success' is not visually similar in 'yes' and 'no'. Therefore, we will keep these features.\n",
    "\n",
    "Going back to the selected features, we will drop the \"poutcome_unknown\" and \"contact_cellular\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToDropLowCor = ['balance','job_student','job_retired','campaign']\n",
    "colToDropHighCor = ['poutcome_unknown','contact_cellular']\n",
    "colToDropLowHighCor = ['balance','job_student','job_retired','campaign','poutcome_unknown','contact_cellular']\n",
    "dfDF_D_L = dfDF.copy()\n",
    "dfDF_D_H = dfDF.copy()\n",
    "dfDF_D_LH = dfDF.copy()\n",
    "dfDF_D_L = dfDF_D_L.drop(colToDropLowCor, axis = 1)\n",
    "dfDF_D_H = dfDF_D_H.drop(colToDropHighCor, axis = 1)\n",
    "dfDF_D_LH = dfDF_D_LH.drop(colToDropLowHighCor, axis = 1)\n",
    "\n",
    "dfDF_Test_D_L = dfDF_Test.copy()\n",
    "dfDF_Test_D_H = dfDF_Test.copy()\n",
    "dfDF_Test_D_LH = dfDF_Test.copy()\n",
    "dfDF_Test_D_L = dfDF_Test_D_L.drop(colToDropLowCor, axis = 1)\n",
    "dfDF_Test_D_H = dfDF_Test_D_H.drop(colToDropHighCor, axis = 1)\n",
    "dfDF_Test_D_LH = dfDF_Test_D_LH.drop(colToDropLowHighCor, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCorrFF(dfDF_Test_D_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCorrFF(dfDF_Test_D_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCorrFF(dfDF_D_LH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fd_train , y_fd_train, x_fd_test , y_fd_test = dfDF.drop(['y'], axis = 1) , dfDF['y'] , dfDF_Test.drop(['y'], axis = 1) , dfDF_Test['y']\n",
    "x_fd_train_L , y_fd_train_L, x_fd_test_L , y_fd_test_L = dfDF_D_L.drop(['y'], axis = 1) , dfDF_D_L['y'] , dfDF_Test_D_L.drop(['y'], axis = 1) , dfDF_Test_D_L['y']\n",
    "x_fd_train_H , y_fd_train_H, x_fd_test_H , y_fd_test_H = dfDF_D_H.drop(['y'], axis = 1) , dfDF_D_H['y'] , dfDF_Test_D_H.drop(['y'], axis = 1) , dfDF_Test_D_H['y']\n",
    "x_fd_train_LH , y_fd_train_LH, x_fd_test_LH , y_fd_test_LH = dfDF_D_LH.drop(['y'], axis = 1) , dfDF_D_LH['y'] , dfDF_Test_D_LH.drop(['y'], axis = 1) , dfDF_Test_D_LH['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fd_train_LH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ANOVA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DF_N = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DF_N = MLPClassifier(random_state=1)\n",
    "KN_clf_DF_N = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DF_N.fit(x_fd_train, y_fd_train)\n",
    "MLP_clf_DF_N.fit(x_fd_train, y_fd_train)\n",
    "KN_clf_DF_N.fit(x_fd_train, y_fd_train)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DF_N.score(x_fd_test, y_fd_test))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DF_N.score(x_fd_test, y_fd_test))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DF_N.score(x_fd_test, y_fd_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFD_pred_test1 = DT_clf_DF_N.predict(x_fd_test)\n",
    "yFD_pred_test2 = MLP_clf_DF_N.predict(x_fd_test)\n",
    "yFD_pred_test3 = KN_clf_DF_N.predict(x_fd_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test, y_pred=yFD_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test, y_pred=yFD_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test, y_pred=yFD_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Drop Low Correltation with Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DF_L = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DF_L = MLPClassifier(random_state=1)\n",
    "KN_clf_DF_L = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DF_L.fit(x_fd_train_L, y_fd_train_L)\n",
    "MLP_clf_DF_L.fit(x_fd_train_L, y_fd_train_L)\n",
    "KN_clf_DF_L.fit(x_fd_train_L, y_fd_train_L)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DF_L.score(x_fd_test_L, y_fd_test_L))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DF_L.score(x_fd_test_L, y_fd_test_L))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DF_L.score(x_fd_test_L, y_fd_test_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFD_pred_test1 = DT_clf_DF_L.predict(x_fd_test_L)\n",
    "yFD_pred_test2 = MLP_clf_DF_L.predict(x_fd_test_L)\n",
    "yFD_pred_test3 = KN_clf_DF_L.predict(x_fd_test_L)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_L, y_pred=yFD_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_L, y_pred=yFD_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_L, y_pred=yFD_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Drop High Correltation with other Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DF_H = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DF_H = MLPClassifier(random_state=1)\n",
    "KN_clf_DF_H = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DF_H.fit(x_fd_train_H, y_fd_train_H)\n",
    "MLP_clf_DF_H.fit(x_fd_train_H, y_fd_train_H)\n",
    "KN_clf_DF_H.fit(x_fd_train_H, y_fd_train_H)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DF_H.score(x_fd_test_H, y_fd_test_H))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DF_H.score(x_fd_test_H, y_fd_test_H))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DF_H.score(x_fd_test_H, y_fd_test_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFD_pred_test1 = DT_clf_DF_H.predict(x_fd_test_H)\n",
    "yFD_pred_test2 = MLP_clf_DF_H.predict(x_fd_test_H)\n",
    "yFD_pred_test3 = KN_clf_DF_H.predict(x_fd_test_H)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_H, y_pred=yFD_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_H, y_pred=yFD_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_H, y_pred=yFD_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Drop Low Correltation with Output and High Correltation with other Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf_DF_LH = DecisionTreeClassifier(random_state=0)\n",
    "MLP_clf_DF_LH = MLPClassifier(random_state=1)\n",
    "KN_clf_DF_LH = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "DT_clf_DF_LH.fit(x_fd_train_LH, y_fd_train_LH)\n",
    "MLP_clf_DF_LH.fit(x_fd_train_LH, y_fd_train_LH)\n",
    "KN_clf_DF_LH.fit(x_fd_train_LH, y_fd_train_LH)\n",
    "print(\"Decision Tree Classifier Accuracy: \", DT_clf_DF_LH.score(x_fd_test_LH, y_fd_test_LH))\n",
    "print(\"MLP Classifier Accuracy: \", MLP_clf_DF_LH.score(x_fd_test_LH, y_fd_test_LH))\n",
    "print(\"KNeighbors Classifier Accuracy: \", KN_clf_DF_LH.score(x_fd_test_LH, y_fd_test_LH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFD_pred_test1 = DT_clf_DF_LH.predict(x_fd_test_LH)\n",
    "yFD_pred_test2 = MLP_clf_DF_LH.predict(x_fd_test_LH)\n",
    "yFD_pred_test3 = KN_clf_DF_LH.predict(x_fd_test_LH)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_LH, y_pred=yFD_pred_test1))\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_LH, y_pred=yFD_pred_test2))\n",
    "print(\"Classification report on Test data with KNN Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_fd_test_LH, y_pred=yFD_pred_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.92|0.92|0.92|0.86|Decision Tree|ANOVA|\n",
    "|1|0.39|0.41|0.40|^|Decision Tree|ANOVA|\n",
    "|0|0.92|0.97|0.94|0.90|MLP|ANOVA|\n",
    "|1|0.61|0.38|0.47|^|MLP|ANOVA|\n",
    "|0|0.91|0.96|0.94|0.88|K-Nearest|ANOVA|\n",
    "|1|0.52|0.30|0.38|^|K-Nearest|ANOVA|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.91|0.96|0.94|0.88|Decision Tree|Drop Low|\n",
    "|1|0.51|0.33|0.40|^|Decision Tree|Drop Low|\n",
    "|0|0.92|0.96|0.94|0.90|MLP|Drop Low|\n",
    "|1|0.59|0.41|0.48|^|MLP|Drop Low|\n",
    "|0|0.91|0.96|0.94|0.88|K-Nearest|Drop Low|\n",
    "|1|0.53|0.31|0.39|^|K-Nearest|Drop Low|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.92|0.92|0.92|0.86|Decision Tree|Drop High|\n",
    "|1|0.40|0.42|0.41|^|Decision Tree|Drop High|\n",
    "|0|0.92|0.97|0.94|0.90|MLP|Drop High|\n",
    "|1|0.62|0.36|0.45|^|MLP|Drop High|\n",
    "|0|0.91|0.96|0.94|0.89|K-Nearest|Drop High|\n",
    "|1|0.52|0.31|0.39|^|K-Nearest|Drop High|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Class Target|Precision|Recall|f1-score|Model Score (Accuracy)|Model|Predict using|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|0.91|0.96|0.94|0.88|Decision Tree|Drop Both|\n",
    "|1|0.52|0.33|0.40|^|Decision Tree|Drop Both|\n",
    "|0|0.92|0.97|0.94|0.90|MLP|Drop Both|\n",
    "|1|0.62|0.36|0.46|^|MLP|Drop Both|\n",
    "|0|0.91|0.96|0.94|0.89|K-Nearest|Drop Both|\n",
    "|1|0.52|0.30|0.38|^|K-Nearest|Drop Both|\n",
    "\n",
    "<br>\n",
    "\n",
    "Results showed that reducing features (that are highly correlated with other features and lowly correlated with the output) after using ANOVA to select features improved the results. However, when using MLP, there was an increase and decrease in both the precision and recall score of class 1. We will consider this incident as a reason for not adjusting the appropriate parameters for the model.\n",
    "\n",
    "Therefore, we will be reducing features to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBase = dfBase.drop(colToDropLowHighCor, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfBase.copy()\n",
    "df.to_csv('preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Model Training Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed.csv')\n",
    "X = df.drop(['y'], axis=1)\n",
    "y = df['y']\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 6 : Trees Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_param = [{'criterion': ['entropy', 'gini', 'log_loss'],\n",
    "               'max_depth': [2,3,4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]},\n",
    "              {'min_samples_leaf': [2,3,4,5]}\n",
    "              ]\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "grid_DT_clf = GridSearchCV(DT_clf\n",
    "                       ,DT_param\n",
    "                       ,cv=10\n",
    "                       ,scoring='f1'\n",
    "                       ,n_jobs=-1)\n",
    "\n",
    "grid_DT_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_DT_clf.best_params_)\n",
    "print(\"f1 :\",grid_DT_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf  = DecisionTreeClassifier(random_state=0,\n",
    "                                 criterion=grid_DT_clf.best_params_['criterion'],\n",
    "                                 max_depth=grid_DT_clf.best_params_['max_depth'],\n",
    "                                 )\n",
    "\n",
    "DT_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = DT_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra Trees Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_param = [{\n",
    "             'criterion': ['entropy', 'gini', 'log_loss'],\n",
    "             'max_depth': [2,3,4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "            }]\n",
    "\n",
    "ET_clf = ExtraTreesClassifier(random_state=0)\n",
    "\n",
    "grid_ET_clf = GridSearchCV(ET_clf\n",
    "                       ,ET_param\n",
    "                       ,cv=4\n",
    "                       ,scoring='f1'\n",
    "                       ,n_jobs=-1)\n",
    "\n",
    "grid_ET_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_ET_clf.best_params_)\n",
    "print(\"f1 :\",grid_ET_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_clf = ExtraTreesClassifier(random_state=0,\n",
    "                            criterion=grid_ET_clf.best_params_['criterion'],\n",
    "                            max_depth=grid_ET_clf.best_params_['max_depth'],\n",
    "                            )\n",
    "\n",
    "ET_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = ET_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_param = [{\n",
    "            # 'n_estimators' : [50, 100, 200],\n",
    "            'criterion': ['entropy', 'gini', 'log_loss'],\n",
    "             'max_depth': [2,3,4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "            }]\n",
    "\n",
    "RF_clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "grid_RF_clf = GridSearchCV(RF_clf\n",
    "                       ,RF_param\n",
    "                       ,cv=4\n",
    "                       ,scoring='f1'\n",
    "                       ,n_jobs=-1)\n",
    "                           \n",
    "grid_RF_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_RF_clf.best_params_)\n",
    "print(\"f1 :\",grid_RF_clf.best_score_)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_clf = RandomForestClassifier(random_state=0,\n",
    "                            criterion=grid_RF_clf.best_params_['criterion'],\n",
    "                            max_depth=grid_RF_clf.best_params_['max_depth'],\n",
    "                            )\n",
    "\n",
    "RF_clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_test = RF_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test))                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_param = [{'loss': ['deviance', 'exponential', 'log_loss'],\n",
    "             'learning_rate' : [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1],\n",
    "             'subsample' : [0.1, 0.2, 0.3, 0.5, 0.7, 1],\n",
    "             'criterion' : ['friedman_mse', 'squared_error', 'mse'],\n",
    "}]\n",
    "\n",
    "GB_clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "grid_GB_clf = GridSearchCV(GB_clf,\n",
    "                           GB_param,\n",
    "                           cv = 4,\n",
    "                           scoring='f1',\n",
    "                            n_jobs=-1,)\n",
    "\n",
    "grid_GB_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_GB_clf.best_params_)\n",
    "print(\"f1 :\",grid_GB_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_clf = GradientBoostingClassifier(random_state=0,\n",
    "                                    criterion=grid_GB_clf.best_params_['criterion'],\n",
    "                                    learning_rate=grid_GB_clf.best_params_['learning_rate'],\n",
    "                                    loss=grid_GB_clf.best_params_['loss'],\n",
    "                                    subsample=grid_GB_clf.best_params_['subsample'],\n",
    "                                    )\n",
    "\n",
    "GB_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = GB_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Decision Tree Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 7 : Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multi-Layer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP_param = [{\n",
    "#             'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "#             'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "#             'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "#             'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "#             'learning_rate_init': [0.001, 0.01, 0.1, 1],\n",
    "            \n",
    "# }]\n",
    "\n",
    "# MLP_clf = MLPClassifier(random_state=1, verbose=True)\n",
    "\n",
    "# grid_MLP_clf = GridSearchCV(MLP_clf,\n",
    "#                            MLP_param,\n",
    "#                            cv = 4,\n",
    "#                            scoring='f1',\n",
    "#                             n_jobs=-1,)\n",
    "                            \n",
    "# grid_MLP_clf.fit(x_train, y_train)\n",
    "# print(\"tuned hpyerparameters :(best parameters) \",grid_MLP_clf.best_params_)\n",
    "# print(\"f1 :\",grid_GB_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_param = [{'hidden_layer_sizes': [(10,),(20,),(30,),(40,),(50,),(60,),(70,),(80,),(90,),(100,),(110,),(120,),(130,),(140,),(150,)],\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "}]\n",
    "\n",
    "MLP_clf = MLPClassifier(random_state=1,\n",
    "                        verbose = True,\n",
    "                        hidden_layer_sizes = (100,),\n",
    "                        activation='relu',\n",
    "                        solver='lbfgs',\n",
    "                        learning_rate='adaptive',\n",
    "                        alpha=0.1,\n",
    "                        )\n",
    "\n",
    "MLP_clf2 = MLPClassifier(random_state=1)\n",
    "\n",
    "MLP_clf.fit(x_train, y_train)\n",
    "MLP_clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = MLP_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with MLP Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Keras Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(units, name, compilerFlag):\n",
    "    model = Sequential(name=name)\n",
    "    if len(units) > 1:\n",
    "        model.add(Dense(units[0], input_shape=(x_train.shape[1],), activation='relu'))\n",
    "        if len(units) > 2:\n",
    "            for unt in units[1:-1]:\n",
    "                model.add(Dense(unt, activation='relu'))\n",
    "        model.add(Dense(units[len(units)-1], activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(units[0], input_shape=(x_train.shape[1],), activation='sigmoid'))\n",
    "    \n",
    "    if compilerFlag == 0:\n",
    "        model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"kerasNN_model1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_263 (Dense)           (None, 100)               700       \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,161\n",
      "Trainable params: 7,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"kerasNN_model2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_268 (Dense)           (None, 20)                140       \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 12)                252       \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 477\n",
      "Trainable params: 477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"kerasNN_model3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_272 (Dense)           (None, 100)               700       \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_275 (Dense)           (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_276 (Dense)           (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,161\n",
      "Trainable params: 7,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"kerasNN_model4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_277 (Dense)           (None, 20)                140       \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 12)                252       \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 477\n",
      "Trainable params: 477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kerasNN_model1 = createModel([100, 50, 25, 5, 1], 'kerasNN_model1', 0)\n",
    "kerasNN_model2 = createModel([20, 12, 6, 1], 'kerasNN_model2', 0)\n",
    "kerasNN_model3 = createModel([100, 50, 25, 5, 1], 'kerasNN_model3', 1)\n",
    "kerasNN_model4 = createModel([20, 12, 6, 1], 'kerasNN_model4', 1)\n",
    "kerasNN_model1.summary()\n",
    "kerasNN_model2.summary()\n",
    "kerasNN_model3.summary()\n",
    "kerasNN_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyKeras_1 = kerasNN_model1.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), verbose=0, use_multiprocessing=True)\n",
    "historyKeras_2 = kerasNN_model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), verbose=0, use_multiprocessing=True)\n",
    "historyKeras_3 = kerasNN_model3.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), verbose=0, use_multiprocessing=True)\n",
    "historyKeras_4 = kerasNN_model4.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), verbose=0, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 1s 2ms/step\n",
      "283/283 [==============================] - 1s 2ms/step\n",
      "283/283 [==============================] - 1s 2ms/step\n",
      "283/283 [==============================] - 1s 2ms/step\n",
      "Classification report on Test data with Keras NN Model 1\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      7965\n",
      "           1       0.57      0.15      0.23      1064\n",
      "\n",
      "    accuracy                           0.89      9029\n",
      "   macro avg       0.73      0.57      0.59      9029\n",
      "weighted avg       0.86      0.89      0.86      9029\n",
      "\n",
      "Classification report on Test data with Keras NN Model 2\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      7965\n",
      "           1       0.57      0.15      0.24      1064\n",
      "\n",
      "    accuracy                           0.89      9029\n",
      "   macro avg       0.73      0.57      0.59      9029\n",
      "weighted avg       0.86      0.89      0.86      9029\n",
      "\n",
      "Classification report on Test data with Keras NN Model 3\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      7965\n",
      "           1       0.00      0.00      0.00      1064\n",
      "\n",
      "    accuracy                           0.88      9029\n",
      "   macro avg       0.44      0.50      0.47      9029\n",
      "weighted avg       0.78      0.88      0.83      9029\n",
      "\n",
      "Classification report on Test data with Keras NN Model 4\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      7965\n",
      "           1       0.00      0.00      0.00      1064\n",
      "\n",
      "    accuracy                           0.88      9029\n",
      "   macro avg       0.44      0.50      0.47      9029\n",
      "weighted avg       0.78      0.88      0.83      9029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kerasNN_model1_pred = (kerasNN_model1.predict(x_test)).round().astype(int)\n",
    "kerasNN_model2_pred = (kerasNN_model2.predict(x_test)).round().astype(int)\n",
    "kerasNN_model3_pred = (kerasNN_model3.predict(x_test)).round().astype(int)\n",
    "kerasNN_model4_pred = (kerasNN_model4.predict(x_test)).round().astype(int)\n",
    "print(\"Classification report on Test data with Keras NN Model 1\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=kerasNN_model1_pred))\n",
    "print(\"Classification report on Test data with Keras NN Model 2\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=kerasNN_model2_pred))\n",
    "print(\"Classification report on Test data with Keras NN Model 3\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=kerasNN_model3_pred))\n",
    "print(\"Classification report on Test data with Keras NN Model 4\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=kerasNN_model4_pred))\n",
    "# np.asarray(np.unique(kerasNN_model1_pred, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 8 : Supervised Neighbors Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNeighbors Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_param = [{ 'n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "             'weights' : ['uniform', 'distance'],\n",
    "             'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "             'leaf_size' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "              }]\n",
    "\n",
    "KN_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "grid_KN_clf = GridSearchCV(KN_clf\n",
    "                       ,KN_param\n",
    "                       ,cv=4\n",
    "                       ,scoring='f1'\n",
    "                       ,n_jobs=-1)\n",
    "\n",
    "grid_KN_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_KN_clf.best_params_)\n",
    "print(\"f1 :\",grid_KN_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_clf = KNeighborsClassifier(n_neighbors=grid_KN_clf.best_params_['n_neighbors'],\n",
    "                              algorithm=grid_KN_clf.best_params_['algorithm'],\n",
    "                              leaf_size=grid_KN_clf.best_params_['leaf_size'],\n",
    "                              weights=grid_KN_clf.best_params_['weights'],\n",
    "                              n_jobs=-1\n",
    "                              )\n",
    "\n",
    "KN_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = KN_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with K Neighbors Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Radius Neighbors Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RN_param = [{ 'radius' : [1, 2],\n",
    "#              'weights' : ['uniform', 'distance'],\n",
    "#              'leaf_size' : [20, 30],\n",
    "#               }]\n",
    "# RN_clf = RadiusNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# grid_RN_clf = GridSearchCV(RN_clf\n",
    "#                        ,RN_param\n",
    "#                        ,cv=4\n",
    "#                        ,scoring='f1'\n",
    "#                        ,n_jobs=-1)\n",
    "\n",
    "# grid_RN_clf.fit(x_train, y_train)\n",
    "# print(\"tuned hpyerparameters :(best parameters) \",grid_RN_clf.best_params_)\n",
    "# print(\"f1 :\",grid_RN_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on Test data with Radius Neighbors Classifier\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      7965\n",
      "           1       0.50      0.01      0.03      1064\n",
      "\n",
      "    accuracy                           0.88      9029\n",
      "   macro avg       0.69      0.51      0.48      9029\n",
      "weighted avg       0.84      0.88      0.83      9029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RN_clf = RadiusNeighborsClassifier(n_jobs=-1 , radius=15)\n",
    "\n",
    "RN_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = RN_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Radius Neighbors Classifier\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Nearest Centroid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'shrink_threshold': 0.34}\n",
      "f1 : 0.2778582239366667\n"
     ]
    }
   ],
   "source": [
    "NC_param = [{ 'shrink_threshold' : [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1],\n",
    "             }]\n",
    "\n",
    "NC_clf = NearestCentroid()\n",
    "\n",
    "grid_NC_clf = GridSearchCV(NC_clf\n",
    "                       ,NC_param\n",
    "                       ,cv=4\n",
    "                       ,scoring='f1'\n",
    "                       ,n_jobs=-1)\n",
    "\n",
    "grid_NC_clf.fit(x_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grid_NC_clf.best_params_)\n",
    "print(\"f1 :\",grid_NC_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on Test data with Nearest Centroid\n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      7965\n",
      "           1       0.24      0.34      0.28      1064\n",
      "\n",
      "    accuracy                           0.79      9029\n",
      "   macro avg       0.57      0.60      0.58      9029\n",
      "weighted avg       0.83      0.79      0.81      9029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NC_clf = NearestCentroid(shrink_threshold=grid_NC_clf.best_params_['shrink_threshold'])\n",
    "\n",
    "NC_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = NC_clf.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Test data with Nearest Centroid\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class columnDropperTransformer():\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None,**transform_params):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toEncode = [[\"single\",\"married\",\"divorced\"], [\"unknown\",\"primary\",\"secondary\",\"tertiary\"]]\n",
    "columnToEncode = ['marital', 'education']\n",
    "\n",
    "LE_pipeline = Pipeline([\n",
    "    (\"LabelEncoder\", LabelEncoder2(toEncode, columnToEncode))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['duration']\n",
    "# drop_transformer = ColumnTransformer(transformers=['drop_columns', 'drop', drop_features], remainder='passthrough')\n",
    "drop_pipeline = Pipeline([\n",
    "    (\"columnDropper\", columnDropperTransformer(drop_features))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpipe = dfOri.copy()\n",
    "\n",
    "numeric_features = dfpipe.select_dtypes(include=['int64', 'float64']).columns.drop('duration')\n",
    "\n",
    "categorical_features = dfpipe.select_dtypes(include=['object']).drop(['y'], axis=1).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "      ('imputer', SimpleImputer(strategy='mean')),\n",
    "      ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant')),\n",
    "      ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) \n",
    "\n",
    "pipeline_tree = Pipeline(steps = [\n",
    "            ('drop', drop_pipeline),\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor',DecisionTreeClassifier())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfpipe.drop(['y'], axis = 1)\n",
    "y = dfpipe['y']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\n",
    "\n",
    "clf_tree = pipeline_tree.fit(x_train, y_train)\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf.fit(clf_tree.transform(x_train), y_train)\n",
    "y_pred = clf_tree.predict(x_test)\n",
    "\n",
    "print(\"Classification report on Train data\\n=======================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "print(\"Confusion matrix on Train data\\n=======================\")\n",
    "\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unknown values\n",
    "dfOri.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1236da61b61376ff1bd0a1c44019e891356fc7f957275c0e89aa284cb1bdefd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
